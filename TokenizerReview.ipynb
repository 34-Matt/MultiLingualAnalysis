{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Analysis\n",
    "Between English and Spanish, the alphabet is identical. The only difference is the addition of the letter ñ (enye) in the Spanish alphabet. Additionally, Spanish implements diacritics on vowels that are not present in English. While not part of the alphabet itself, these diacritics result in different character representations.It is therefore expected that individual character tokenizing should exist in the same domain for all tokenizers.\n",
    "\n",
    "The Japanese alphabet consists of different characters, consisting of three writing systems: kanji, hiragana, and katakana. Both hiragana and katakana are syllabary like the English alaphabet, but each containing there own set of characters. Kanji is a logographic writing system, sharing its characters with the chinese writing system.\n",
    "\n",
    "### Base Byte Pair Encoding\n",
    "English and Spanish characters begin at U+0041, with additional Spanish characters beginning at U+00C1.\n",
    "Hiragana begins at U+3041, katakana at U+30A0, and kanji at U+4E00.\n",
    "For byte pair encoding, all characters below U+00FF are treated as single characters. This means that English and Spanish characters are treated as single characters when initializing the tokenizer. Meanwhile, Japanese characters require two bytes to be represented. This means that either the tokenizer will need to learn a new token to represent each character, or the model will need to learn how multiple tokens can be combined to represent a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "a: 0x0 0x61 (97)\tb: 0x0 0x62 (98)\n",
      "c: 0x0 0x63 (99)\tx: 0x0 0x78 (120)\n",
      "y: 0x0 0x79 (121)\tz: 0x0 0x7a (122)\n",
      "A: 0x0 0x41 (65)\tB: 0x0 0x42 (66)\n",
      "C: 0x0 0x43 (67)\tX: 0x0 0x58 (88)\n",
      "Y: 0x0 0x59 (89)\tZ: 0x0 0x5a (90)\n",
      "\n",
      "Spanish\n",
      "a: 0x0 0x61 (97)\tb: 0x0 0x62 (98)\n",
      "ñ: 0x0 0xf1 (241)\ty: 0x0 0x79 (121)\n",
      "z: 0x0 0x7a (122)\tA: 0x0 0x41 (65)\n",
      "B: 0x0 0x42 (66)\tÑ: 0x0 0xd1 (209)\n",
      "Y: 0x0 0x59 (89)\tZ: 0x0 0x5a (90)\n",
      "\n",
      "Japanese\n",
      "あ: 0x30 0x42 (12354)\tい: 0x30 0x44 (12356)\n",
      "う: 0x30 0x46 (12358)\tえ: 0x30 0x48 (12360)\n",
      "お: 0x30 0x4a (12362)\tア: 0x30 0xa2 (12450)\n",
      "イ: 0x30 0xa4 (12452)\tウ: 0x30 0xa6 (12454)\n",
      "エ: 0x30 0xa8 (12456)\tオ: 0x30 0xaa (12458)\n",
      "上: 0x4e 0xa (19978)\t中: 0x4e 0x2d (20013)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_characters = \"abcxyzABCXYZ\"\n",
    "spanish_characters = \"abñyzABÑYZ\"\n",
    "japanese_characters = \"あいうえおアイウエオ上中\"\n",
    "\n",
    "example_characters = {\n",
    "    \"English\": english_characters,\n",
    "    \"Spanish\": spanish_characters,\n",
    "    \"Japanese\": japanese_characters,\n",
    "    }\n",
    "\n",
    "def get_bytes(character):\n",
    "    char_number = ord(character)\n",
    "    upper, lower = divmod(char_number, 0x100)\n",
    "    return f\"{hex(upper)} {hex(lower)}\"\n",
    "\n",
    "for language, characters in example_characters.items():\n",
    "    print(language)\n",
    "    count = 0\n",
    "    for character in characters:\n",
    "        end_char = \"\\n\" if count%2 else \"\\t\"\n",
    "        print(f\"{character}: {get_bytes(character)} ({ord(character)})\", end=end_char)\n",
    "        count += 1\n",
    "    print(\"\\n\" if count%2 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmdst\\.conda\\envs\\LLM_Class\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenizer_character_test(model):\n",
    "    for language, characters in example_characters.items():\n",
    "        print(language)\n",
    "        count = 0\n",
    "        for character in characters:\n",
    "            end_char = \"\\n\" if count%2 else \"\\t\\t\"\n",
    "            tokenized = model.encode(character)\n",
    "            print(f\"{character}: {tokenized}\", end=end_char)\n",
    "            count += 1\n",
    "        print(\"\\n\" if count%2 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT-2 Tokenizer\n",
    "\n",
    "GPT-2 is the model created by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "a: [64]\t\tb: [65]\n",
      "c: [66]\t\tx: [87]\n",
      "y: [88]\t\tz: [89]\n",
      "A: [32]\t\tB: [33]\n",
      "C: [34]\t\tX: [55]\n",
      "Y: [56]\t\tZ: [57]\n",
      "\n",
      "Spanish\n",
      "a: [64]\t\tb: [65]\n",
      "ñ: [12654]\t\ty: [88]\n",
      "z: [89]\t\tA: [32]\n",
      "B: [33]\t\tÑ: [127, 239]\n",
      "Y: [56]\t\tZ: [57]\n",
      "\n",
      "Japanese\n",
      "あ: [40948]\t\tい: [18566]\n",
      "う: [29557]\t\tえ: [2515, 230]\n",
      "お: [2515, 232]\t\tア: [11839]\n",
      "イ: [11482]\t\tウ: [16165]\n",
      "エ: [23544]\t\tオ: [20513]\n",
      "上: [41468]\t\t中: [40792]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPT2_Tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=config['HUGGING'])\n",
    "\n",
    "tokenizer_character_test(GPT2_Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite Ñ being a single character in the Spanish alphabet, the GPT-2 tokenizer treats it as two characters.\n",
    "This is likely because GPT-2 Tokenizer was created using traditional ASCII encoding instead of extended ASCII encoding. Traditionally, ASCII only supported up to U+007F, which is the first 128 characters of the Unicode standard. Extended ASCII encoding supports up to U+00FF, which includes characters like ñ.\n",
    "Looking at the byte pairs, we can see what the tokenizer is representing.\n",
    "\n",
    "This similarly occurs with the Japanese characters, though was expected from the start since Japanese characters require two bytes to be represented. The tokenizer learns byte pairs for many characters, but not all. This is likely due to the characters being too rare to be represented in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google T5 Transformer\n",
    "\n",
    "T5 is the model created by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmdst\\.conda\\envs\\LLM_Class\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mmdst\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "a: [2, 236746]\t\tb: [2, 236763]\n",
      "c: [2, 236755]\t\tx: [2, 236781]\n",
      "y: [2, 236762]\t\tz: [2, 236802]\n",
      "A: [2, 236776]\t\tB: [2, 236799]\n",
      "C: [2, 236780]\t\tX: [2, 236917]\n",
      "Y: [2, 236874]\t\tZ: [2, 236953]\n",
      "\n",
      "Spanish\n",
      "a: [2, 236746]\t\tb: [2, 236763]\n",
      "ñ: [2, 237168]\t\ty: [2, 236762]\n",
      "z: [2, 236802]\t\tA: [2, 236776]\n",
      "B: [2, 236799]\t\tÑ: [2, 240643]\n",
      "Y: [2, 236874]\t\tZ: [2, 236953]\n",
      "\n",
      "Japanese\n",
      "あ: [2, 237268]\t\tい: [2, 236985]\n",
      "う: [2, 237187]\t\tえ: [2, 237495]\n",
      "お: [2, 237328]\t\tア: [2, 237254]\n",
      "イ: [2, 237118]\t\tウ: [2, 237656]\n",
      "エ: [2, 237746]\t\tオ: [2, 237705]\n",
      "上: [2, 237152]\t\t中: [2, 237103]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T5_Tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\", token=config['HUGGING'])\n",
    "\n",
    "tokenizer_character_test(T5_Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appearance of 2 tokens in most characters was originally unexpected, but comes into how the tokenizer processes text. Token ID 2 refers to the start of a word.\n",
    "\n",
    "Additionally, the token IDs are not ordered alphabetically and are larger for the english characters compared to the GPT-2 tokenizer. This means another method was used to initiate the byte-pair encoding instead of UTF or ASCII encoding. The larger token IDs is because the T5 tokenizer begins with various special tokens, shifting the alphabet to a higher token ID. A few of these special tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t<pad>\n",
      "1\t<eos>\n",
      "2\t<bos>\n",
      "3\t<unk>\n",
      "4\t<mask>\n",
      "5\t[multimodal]\n",
      "6\t<unused0>\n",
      "7\t<unused1>\n",
      "8\t<unused2>\n",
      "9\t<unused3>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, end=\"\\t\")\n",
    "    print(T5_Tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analysis\n",
    "The next step for byte pair encoding is to tokenize multiple characters into single tokens. As this is based on the frequency of characters, there is unlikely any similarity between tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = [\"Hello\", \"world\", \"John\"]\n",
    "spanish_words = [\"Hola\", \"mundo\", \"Juan\"]\n",
    "japanese_words = [\"おはよう\", \"世界\", \"ジョン\"]\n",
    "\n",
    "example_words = {\n",
    "    \"English\": english_words,\n",
    "    \"Spanish\": spanish_words,\n",
    "    \"Japanese\": japanese_words,\n",
    "}\n",
    "\n",
    "def tokenizer_word_test(model):\n",
    "    for language, characters in example_words.items():\n",
    "        print(language)\n",
    "        count = 0\n",
    "        for character in characters:\n",
    "            tokenized = model.encode(character)\n",
    "            print(f\"{character}: {tokenized}\")\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Hello: [15496]\n",
      "world: [6894]\n",
      "John: [7554]\n",
      "Spanish\n",
      "Hola: [39, 5708]\n",
      "mundo: [20125, 78]\n",
      "Juan: [41, 7258]\n",
      "Japanese\n",
      "おはよう: [2515, 232, 31676, 1792, 230, 29557]\n",
      "世界: [10310, 244, 45911, 234]\n",
      "ジョン: [21091, 1209, 100, 6527]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_word_test(GPT2_Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest point of interest is that most of the Japanese words tested had multiple tokens representing a relatively common word. This result is not expected for GPT-3 and above, as one of the major improvements between the two models is the a siginifant increase of vocabulary size from the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Hello: [2, 9259]\n",
      "world: [2, 12392]\n",
      "John: [2, 12720]\n",
      "Spanish\n",
      "Hola: [2, 21529]\n",
      "mundo: [2, 223428]\n",
      "Juan: [2, 76777]\n",
      "Japanese\n",
      "おはよう: [2, 220844]\n",
      "世界: [2, 12811]\n",
      "ジョン: [2, 104950]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_word_test(T5_Tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
